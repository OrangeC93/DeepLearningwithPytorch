{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Tabular data \n",
    "A spreadsheet, in a CSV (comma-separated values) file, or in a database.\n",
    "\n",
    "Whatever the medium, this data is a table containing one row per sample (or record), in which col- umns contain one piece of information about the sample.\n",
    "\n",
    "## Interal, ordinal, and categorical values\n",
    "1. continuous values: these values are the most intuitive when represented as numbers.\n",
    "\n",
    "2. ordinal values: the strict ordering of continuous values remains, but the fixed relationship between values no longer applies. \n",
    "\n",
    "3. categorical values: these values have neither ordering nor numerical meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_loading_data.ipynb \u001b[34mdata\u001b[m\u001b[m                 \u001b[34mpictures\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "wine_path = \"data/winequality-white.csv\"\n",
    "\n",
    "# read data\n",
    "wineq_numpy = np.loadtxt(wine_path, \n",
    "                         dtype=np.float32, \n",
    "                         delimiter=\";\",\n",
    "                         skiprows=1)\n",
    "\n",
    "# 2d array, 32-bit floating point\n",
    "wineq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 12)\n",
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    }
   ],
   "source": [
    "# check all data has been read\n",
    "col_list = next(csv.reader(open(wine_path), \n",
    "                           delimiter=';'))\n",
    "\n",
    "print(wineq_numpy.shape)\n",
    "print(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4898, 12]) torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# convert numpy to pytorch\n",
    "wineq = torch.from_numpy(wineq_numpy)\n",
    "\n",
    "print(wineq.shape, wineq.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4898, 11])\n",
      "torch.Size([4898])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6,  ..., 6, 7, 6])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select all rows and all columns except the last\n",
    "data = wineq[:, :-1]\n",
    "print(data.shape)\n",
    "\n",
    "# select all rows and the last column\n",
    "target = wineq[:, -1]\n",
    "print(target.shape)\n",
    "# treat lable as an integer vector of scores\n",
    "target = wineq[:, -1].long()\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Munipulation\n",
    "1. one hot target column: keeping wine-quality scores in an integer vector of scores induces an ordering of the scores, which may be appropriate in this case because a score of 1 is lower than a score of 4. \n",
    "2. nomalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot target column\n",
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomalize data\n",
    "data_mean = torch.mean(data, dim=0)\n",
    "data_var = torch.var(data, dim=0)\n",
    "data_normalized = (data - data_mean) / torch.sqrt(data_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAD \n",
    "1. Get information about wines grouped into good, middling, and bad categories. At first glance, the bad wines seem to have higher total sulfur dioxide, among other differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4898]) torch.bool tensor(20)\n",
      "torch.Size([20, 11])\n"
     ]
    }
   ],
   "source": [
    "bad_indexes = torch.le(target, 3)\n",
    "print(bad_indexes.shape, bad_indexes.dtype, bad_indexes.sum())\n",
    "\n",
    "bad_data = data[bad_indexes]\n",
    "print(bad_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 fixed acidity          7.60   6.86   6.68\n",
      " 1 volatile acidity       0.33   0.28   0.28\n",
      " 2 citric acid            0.34   0.33   0.33\n",
      " 3 residual sugar         6.39   6.42   5.63\n",
      " 4 chlorides              0.05   0.05   0.04\n",
      " 5 free sulfur dioxide   53.33  35.18  36.63\n",
      " 6 total sulfur dioxide 170.60 138.70 125.88\n",
      " 7 density                0.99   0.99   0.99\n",
      " 8 pH                     3.19   3.19   3.22\n",
      " 9 sulphates              0.47   0.49   0.49\n",
      "10 alcohol               10.34  10.47  11.65\n"
     ]
    }
   ],
   "source": [
    "bad_data = data[torch.le(target,3)]\n",
    "mid_data = data[torch.gt(target,3) & torch.le(target,7)]\n",
    "good_data = data[torch.gt(target,7)]\n",
    "\n",
    "bad_mean = torch.mean(bad_data, dim=0)\n",
    "mid_mean = torch.mean(mid_data, dim=0)\n",
    "good_mean = torch.mean(good_data, dim=0)\n",
    "\n",
    "for i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):\n",
    "    print('{:2} {:20} {:6.2f} {:6.2f} {:6.2f}'.format(i, *args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set the threshold adn get the indexes in which the total sulfur dioxide column is below the mid point you calculated earlier. The result implies that slightly more than half of the wines are going to be hight quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4898]) torch.bool tensor(2727)\n"
     ]
    }
   ],
   "source": [
    "total_sulfur_threshold = 141.83\n",
    "total_sulfur_data = data[:,6]\n",
    "predicted_indexes = torch.lt(total_sulfur_data, \n",
    "                             total_sulfur_threshold)\n",
    "\n",
    "print(predicted_indexes.shape, \n",
    "      predicted_indexes.dtype, \n",
    "      predicted_indexes.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we got 500 more good wines than the previous predicted predicted, which is the hard evidence that the threshold isn't perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4898]) torch.bool tensor(3258)\n"
     ]
    }
   ],
   "source": [
    "actual_indexes = torch.gt(target, 5)\n",
    "print(actual_indexes.shape, \n",
    "      actual_indexes.dtype, \n",
    "      actual_indexes.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well the predictions line up with the actual rankings?\n",
    "\n",
    "- You got around 2,000 wines right! \n",
    "- Because you had 2,700 wines predicted, a 74% chance exists that if you predict a wine to be high-quality, it is. \n",
    "- Unfortunately, you have 3,200 good wines and identified only 61% of them. Well, we guess you got what you signed up for; \n",
    "- The result is barely better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 0.74000733406674 0.6193984039287906\n"
     ]
    }
   ],
   "source": [
    "n_matches = torch.sum(actual_indexes & predicted_indexes).item()\n",
    "n_predicted = torch.sum(predicted_indexes).item()\n",
    "n_actual = torch.sum(actual_indexes).item() \n",
    "\n",
    "print(n_matches, \n",
    "      n_matches / n_predicted, \n",
    "      n_matches / n_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Time Series\n",
    "\n",
    "Here it’s important to an idea of how a time series is laid out and how to wrangle the data into a form that a network will digest. [dataset](https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/data/p1ch4/bike-sharing-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_loading_data.ipynb \u001b[34mdata\u001b[m\u001b[m                 \u001b[34mpictures\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/bike-sharing-dataset/hour-fixed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bikes_numpy = np.loadtxt(data_path,\n",
    "                         dtype=np.float32,\n",
    "                         delimiter=\",\",\n",
    "                         skiprows=1,\n",
    "                         converters={1: lambda x: float(x[8:10])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17520, 17]) (17, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "# That’s 17,520 hours, 17 columns. \n",
    "print(bikes.shape, bikes.stride())\n",
    "# print(bikes.storage()[0:24])\n",
    "# bikes[0:24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reshape the data to have three axes (day, hour, and 17 columns)\n",
    "\n",
    "1. N x C x L tensor: \n",
    "    - C remains your 17 channels\n",
    "    - L would be 24, one per hour of the day\n",
    "    - The first column is the index (the global ordering of the data); the second is the date; the sixth is the time of day. \n",
    "    \n",
    "2. Attention: Calling view on a tensor returns a new tensor that changes the number of dimensions and the striding information without changing the storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([730, 24, 17]) (408, 17, 1)\n",
      "torch.Size([730, 17, 24]) (408, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "# step 1:\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "print(daily_bikes.shape, daily_bikes.stride())\n",
    "# print(daily_bikes.storage()[0:24])\n",
    "# daily_bikes[0]\n",
    "\n",
    "# step 2:\n",
    "daily_bikes = daily_bikes.transpose(1, 2)\n",
    "print(daily_bikes.shape, daily_bikes.stride())\n",
    "# print(daily_bikes.storage()[0:24])\n",
    "# daily_bikes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert ordinal variable: weather situation into one-hot categorical variable\n",
    "1. First, initialize a zero-filled matrix with a number of rows equal to the number of hours in the day and a number of columns equal to the number of weather levels.\n",
    "2. Then, scatter ones into our matrix according to the corresponding level at each row.\n",
    "3. Last, concatenate your matrix to your original data set, using the cat function.\n",
    "\n",
    "\n",
    "Then we have to done the same thing with the reshaped daily_bikes tensor.\n",
    "1. It's shaped (B, C, L), where L = 24. First, create the zero tensors, with the same B and L but with the number of added columns as C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  6.0000,\n",
       "          0.0000,  1.0000,  0.2400,  0.2879,  0.8100,  0.0000,  3.0000, 13.0000,\n",
       "         16.0000,  1.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use first day as an example\n",
    "# step1:\n",
    "first_day = bikes[:24].long()\n",
    "weather_onehot = torch.zeros(first_day.shape[0], 4)\n",
    "print(first_day[:,9])\n",
    "\n",
    "# step2:\n",
    "weather_onehot.scatter_(\n",
    "    dim=1,\n",
    "    index=first_day[:,9].unsqueeze(1) - 1, # You’re decreasing the values by 1 because the weather situation ranges from 1 to 4, whereas indices are 0-based.\n",
    "    value=1.0)\n",
    "\n",
    "#step3:\n",
    "torch.cat((bikes[:24], weather_onehot), 1)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([730, 4, 24])\n",
      "torch.Size([730, 4, 24])\n",
      "torch.Size([730, 21, 24])\n"
     ]
    }
   ],
   "source": [
    "# apply to whole data set\n",
    "# step 1:\n",
    "daily_weather_onehot = torch.zeros(daily_bikes.shape[0], \n",
    "                                   4, \n",
    "                                   daily_bikes.shape[2])\n",
    "print(daily_weather_onehot.shape)\n",
    "# step 2:\n",
    "daily_weather_onehot.scatter_(\n",
    "    dim=1,\n",
    "    index=daily_bikes[:,9,:].long().unsqueeze(1)-1,\n",
    "    value=1.0)\n",
    "print(daily_weather_onehot.shape)\n",
    "# step 3:\n",
    "daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)\n",
    "print(daily_bikes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or treat ordinal variable: weather situation as a special values of a continuous variable.\n",
    "\n",
    "So we need to rescale the variable into [0.0, 1] as other variables (or the [-1.0, 1.0] interval is something that you’ll want to do for all quantitative variables,)\n",
    "\n",
    "For example column 10: temperature, it's beneficial to rescale this variable into [0.0, 1.0] or [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale into [0.0, 1.0]\n",
    "temp = daily_bikes[:, 10, :]\n",
    "temp_min  = torch.min(temp)\n",
    "temp_max = torch.max(temp)\n",
    "\n",
    "daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - temp_min) / (temp_max - temp_min)\n",
    "\n",
    "# scale into[-1.0, 1.0]\n",
    "temp = daily_bikes[:, 10, :]\n",
    "daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - torch.mean(temp)) / torch.std(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Text\n",
    "\n",
    "Networks operate on text at two levels: at character level, by processing one charac- ter at a time, and at word level, in which individual words are the finest-grained enti- ties seen by the network. The technique you use to encode text information into tensor form is the same whether you operate at character level or at word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/1342-0.txt', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Impossible, Mr. Bennet, impossible, when I am not acquainted with him\n",
      "torch.Size([70, 128])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# split text into a list of lines \n",
    "# character level encoding\n",
    "lines = text.split('\\n') \n",
    "line = lines[200]\n",
    "print(line)\n",
    "\n",
    "# create a tensor that can hold the total number of one-hot encoded charaters for the whole line\n",
    "letter_tensor = torch.zeros(len(line), 128)\n",
    "print(letter_tensor.size())\n",
    "\n",
    "for i, letter in enumerate(line.lower().strip()):\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
    "    letter_tensor[i][letter_index] = 1\n",
    "\n",
    "print(letter_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a mapping of words to indexes in your encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(input_str):\n",
    "    puctuation = '.,;:\"!?”“_-'\n",
    "    word_list = input_str.lower().replace('\\n',' ').split()\n",
    "    word_list = [word.strip(puctuation) for word in word_list]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Impossible, Mr. Bennet, impossible, when I am not acquainted with him\n",
      "['impossible', 'mr', 'bennet', 'impossible', 'when', 'i', 'am', 'not', 'acquainted', 'with', 'him']\n"
     ]
    }
   ],
   "source": [
    "words_in_line = clean_words(line) \n",
    "\n",
    "print(line)\n",
    "print(words_in_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7260, 3394)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = sorted(set(clean_words(text)))\n",
    "word2index_dict = {word: i for (i, word) in enumerate(word_list)}\n",
    "len(word2index_dict), word2index_dict['impossible']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now focus on your sentence. Break it into words and one-hot encode it—that is, populate a tensor with one one-hot encoded vector per word. Create an empty vector,\n",
    "and assign the one-hot encoded values of the word in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 3394 impossible\n",
      " 1 4305 mr\n",
      " 2  813 bennet\n",
      " 3 3394 impossible\n",
      " 4 7078 when\n",
      " 5 3315 i\n",
      " 6  415 am\n",
      " 7 4436 not\n",
      " 8  239 acquainted\n",
      " 9 7148 with\n",
      "10 3215 him\n"
     ]
    }
   ],
   "source": [
    "word_tensor = torch.zeros(len(words_in_line), len(word2index_dict)) \n",
    "for i, word in enumerate(words_in_line):\n",
    "    word_index = word2index_dict[word] \n",
    "    word_tensor[i][word_index] = 1\n",
    "    print('{:2} {:4} {}'.format(i, word_index, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Text embeddings\n",
    "\n",
    "They're the essential tools when a large number of entries in a set has to be represented with numeric vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Images\n",
    "1. read one dog image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "img_arr = imageio.imread('data/bobby.jpg')\n",
    "print(img_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, img is a NumPy array-like object with three dimensions: two spatial dimensions (width and height) and a third dimension corresponding to the channels red, green, and blue.\n",
    "\n",
    "\n",
    "PyTorch modules that deal with image data require tensors to be laid out as C x H x W (chan- nels, height, and width, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1280, 720])\n"
     ]
    }
   ],
   "source": [
    "img = torch.from_numpy(img_arr)\n",
    "out = torch.transpose(img, 0, 2)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. read multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_loading_data.ipynb      \u001b[34mpictures\u001b[m\u001b[m\r\n",
      "\u001b[34mdata\u001b[m\u001b[m                      \u001b[34mraw.githubusercontent.com\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch consists of 100 RGB images 256 pixels in height and 256 pixels in width\n",
    "batch_size = 100\n",
    "batch = torch.zeros(100, 4, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/awang/Documents/Deep_Learning_with_PyTorch/Chapter3/code/data/'\n",
    "filenames = [name for name in os.listdir(data_dir) if str.endswith(name, '.png')]\n",
    "for i, filename in enumerate(filenames):\n",
    "    img_arr = imageio.imread('data/' + filename)\n",
    "    batch[i] = torch.transpose(torch.from_numpy(img_arr), 0 ,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical thing that you’ll want to do is cast a tensor to floating-point and normalize the values of the pixels. Casting to floating-point is easy, but normalization is trickier, as it depends on what range of the input you decide should lie between 0 and 1 (or –1 and 1). \n",
    "- One possibility is to divide the values of pixels by 255 (the maximum representable number in 8-bit unsigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.float()\n",
    "batch /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another possibility is to compute mean and standard deviation of the input data and scale it so that the output has zero mean and unit standard deviation across each channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = batch.shape[1] \n",
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:, c])\n",
    "    batch[:, c] = (batch[:, c] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can perform several other operations on inputs, including geometric transfor- mations such as rotation, scaling, and cropping. These operations may help with training or may be required to make an arbitrary input conform to the input requirements of a network, such as the size of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Volumetric data\n",
    "\n",
    "For now, it suffices to say that no fundamental difference exists between a tensor that stores volumetric data and one that stores image data. You have an extra dimension, depth, after the channel dimension, leading to a 5D tensor of shape N x C x D x H x W.\n",
    "\n",
    "![figure](pictures/ctscans.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_loading_data.ipynb      \u001b[34mpictures\u001b[m\u001b[m\r\n",
      "\u001b[34mdata\u001b[m\u001b[m                      \u001b[34mraw.githubusercontent.com\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample CT scan by using the volread function in the imageio module, which takes a directory as argument and assembles all DICOM (Digital Imaging Communica- tion and Storage) files10 in a series in a NumPy 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 99/99  (100.0%)\n",
      "(99, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "dir_path = \"/Users/awang/Documents/Deep_Learning_with_PyTorch/Chapter3/code/data/volumetric-dicom/2-LUNG 3.0  B70f-04083\"\n",
    "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
    "print(vol_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 512, 512])\n",
      "torch.Size([512, 512, 99])\n",
      "torch.Size([1, 512, 512, 99])\n"
     ]
    }
   ],
   "source": [
    "vol = torch.from_numpy(vol_arr).float()\n",
    "print(vol.shape)\n",
    "vol = torch.transpose(vol, 0, 2)\n",
    "print(vol.shape)\n",
    "# make room for the channel dimension by using unsqueeze\n",
    "vol = torch.unsqueeze(vol, 0)\n",
    "print(vol.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to deal with these pictures?????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
